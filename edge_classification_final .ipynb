{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b32403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset  # not the one from PyG!\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from typing import List, Union\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bba04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path: Path):\n",
    "        super().__init__()\n",
    "        self.graphs = list(path.glob(\"**/*.pt\"))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(self.graphs[idx])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "dataset = MyDataset(Path(\"/home/hrc/gsoc/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a706abee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[272, 6], edge_index=[2, 2370], edge_attr=[2370, 4], y=[2370])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503d66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75fbc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cb4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reproduced from somewhere on GitHub\n",
    "def make_mlp(\n",
    "    input_size: int,\n",
    "    sizes: List,\n",
    "    hidden_activation: str = \"SiLU\",\n",
    "    output_activation: str = None,\n",
    ") -> nn.Sequential:\n",
    "    \"\"\"Construct an MLP with specified fully-connected layers.\"\"\"\n",
    "    hidden_activation = getattr(nn, hidden_activation)\n",
    "    if output_activation is not None:\n",
    "        output_activation = getattr(nn, output_activation)\n",
    "    layers = []\n",
    "    n_layers = len(sizes)\n",
    "    sizes = [input_size] + sizes\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(n_layers - 1):\n",
    "        layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        layers.append(hidden_activation())\n",
    "        \n",
    "    # Final layer\n",
    "    layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "    if output_activation is not None:\n",
    "        layers.append(output_activation())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02de8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import add_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0cdcd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMessagePassing(MessagePassing):\n",
    "    def __init__(self, num_input_node_features,num_input_edge_features,num_hidden,num_output_node_features):\n",
    "        super().__init__(aggr='add')\n",
    "        in_channels = 2 * num_input_node_features + num_input_edge_features\n",
    "        hidden_channels = [in_channels] * num_hidden\n",
    "        self.phi = make_mlp(in_channels,hidden_channels,\"ReLU\",\"Sigmoid\")\n",
    "        self.gamma = make_mlp(num_input_node_features + hidden_channels[-1], hidden_channels +[num_output_node_features],\"ReLU\",\"Sigmoid\")\n",
    "    \n",
    "    def forward(self,x,edge_index,edge_attr):\n",
    "        out = self.propagate(edge_index,x=x,edge_attr=edge_attr)\n",
    "        return out\n",
    "    \n",
    "    def message(self,x_i,x_j,edge_attr):\n",
    "        inp = torch.concat([x_i,x_j,edge_attr],dim=-1)\n",
    "        return self.phi(inp)\n",
    "    \n",
    "    def update(self,aggr_out,x):\n",
    "        inp = torch.concat([aggr_out,x],dim=-1)\n",
    "        return self.gamma(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ea0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,hidden_channels):\n",
    "        super().__init__()\n",
    "#         self.conv1 = GATConv(-1, hidden_channels)\n",
    "#         self.conv2 = GATConv(-1, 6)\n",
    "        self.conv1 = MyMessagePassing(6,4,3,16) # This might not be the best design for Message Passing Layer, but it is just an optional demo\n",
    "        self.conv2 = MyMessagePassing(16,4,3,6)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16,6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index,edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index,edge_attr)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        fc_inp = torch.cat([x[edge_index[0]],x[edge_index[1]],edge_attr],dim=-1)\n",
    "        out = self.fc(fc_inp)\n",
    "        out = F.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a8dd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93881ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    \n",
    "    loss_all = 0\n",
    "    for graph_data in loader:\n",
    "\n",
    "        output = model(graph_data)        \n",
    "        loss = criterion(output.reshape(-1), graph_data.y)\n",
    "        loss_all += loss.item()\n",
    "        print(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f59be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrc/anaconda3/envs/gnn/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6139814257621765\n",
      "0.5935847163200378\n",
      "0.5744291543960571\n",
      "0.5559366345405579\n",
      "0.5401993989944458\n",
      "0.5223404765129089\n",
      "0.5070285797119141\n",
      "0.4892081916332245\n",
      "0.4832633137702942\n",
      "0.46790605783462524\n",
      "0.4477982223033905\n",
      "0.4390258193016052\n",
      "0.427238404750824\n",
      "0.42406028509140015\n",
      "0.41388407349586487\n",
      "0.4172343909740448\n",
      "0.40494853258132935\n",
      "0.4035087525844574\n",
      "0.3976539075374603\n",
      "0.396498441696167\n",
      "0.38354969024658203\n",
      "0.38267773389816284\n",
      "0.37266334891319275\n",
      "0.37485218048095703\n",
      "0.37360572814941406\n",
      "0.3542994558811188\n",
      "0.342974454164505\n",
      "0.3443525731563568\n",
      "0.3403206169605255\n",
      "0.3476988971233368\n",
      "0.33552122116088867\n",
      "0.34299296140670776\n",
      "0.34122344851493835\n",
      "0.3293152451515198\n",
      "0.3382563889026642\n",
      "0.34206247329711914\n",
      "0.3217293620109558\n",
      "0.3369494676589966\n",
      "0.32852599024772644\n",
      "0.3222833275794983\n",
      "16.475583344697952\n",
      "0.33540210127830505\n",
      "0.3312946557998657\n",
      "0.33396658301353455\n",
      "0.33110126852989197\n",
      "0.33217787742614746\n",
      "0.32950201630592346\n",
      "0.32790839672088623\n",
      "0.32243672013282776\n",
      "0.3336218595504761\n",
      "0.3326578438282013\n",
      "0.32200002670288086\n",
      "0.3258647322654724\n",
      "0.3226947784423828\n",
      "0.32687658071517944\n",
      "0.3227154016494751\n",
      "0.32991740107536316\n",
      "0.3223024904727936\n",
      "0.32413989305496216\n",
      "0.32185155153274536\n",
      "0.324028342962265\n",
      "0.3164654076099396\n",
      "0.3199686110019684\n",
      "0.31439101696014404\n",
      "0.319132536649704\n",
      "0.3228149116039276\n",
      "0.31195876002311707\n",
      "0.30896997451782227\n",
      "0.31765133142471313\n",
      "0.31628507375717163\n",
      "0.31209176778793335\n",
      "0.3098839223384857\n",
      "0.31940600275993347\n",
      "0.3193592429161072\n",
      "0.30882617831230164\n",
      "0.31473931670188904\n",
      "0.3185552954673767\n",
      "0.3020424246788025\n",
      "0.3151130676269531\n",
      "0.30650803446769714\n",
      "0.3029176592826843\n",
      "12.829541057348251\n",
      "0.3129209280014038\n",
      "0.30959856510162354\n",
      "0.3113110363483429\n",
      "0.3095099925994873\n",
      "0.31389573216438293\n",
      "0.30836769938468933\n",
      "0.3129819929599762\n",
      "0.30420976877212524\n",
      "0.31904304027557373\n",
      "0.3179663121700287\n",
      "0.3050170838832855\n",
      "0.30815234780311584\n",
      "0.3057118058204651\n",
      "0.3080197870731354\n",
      "0.30330076813697815\n",
      "0.310045063495636\n",
      "0.30419573187828064\n",
      "0.305520623922348\n",
      "0.3034040331840515\n",
      "0.3045807480812073\n",
      "0.29770365357398987\n",
      "0.3003159165382385\n",
      "0.29482582211494446\n",
      "0.29915785789489746\n",
      "0.302694708108902\n",
      "0.29265812039375305\n",
      "0.29000070691108704\n",
      "0.29799196124076843\n",
      "0.2969650328159332\n",
      "0.29265713691711426\n",
      "0.290749192237854\n",
      "0.29925471544265747\n",
      "0.29870161414146423\n",
      "0.2893303334712982\n",
      "0.2948146462440491\n",
      "0.2981815040111542\n",
      "0.2835504114627838\n",
      "0.2947137653827667\n",
      "0.2870156466960907\n",
      "0.2850814759731293\n",
      "12.064117282629013\n",
      "0.29275357723236084\n",
      "0.28990626335144043\n",
      "0.29172149300575256\n",
      "0.2893707752227783\n",
      "0.2905123829841614\n",
      "0.2885609567165375\n",
      "0.28657883405685425\n",
      "0.28201642632484436\n",
      "0.2914685606956482\n",
      "0.29040977358818054\n",
      "0.2817849814891815\n",
      "0.285326212644577\n",
      "0.28247448801994324\n",
      "0.28572559356689453\n",
      "0.28186526894569397\n",
      "0.28796589374542236\n",
      "0.2818441390991211\n",
      "0.28311583399772644\n",
      "0.2823588252067566\n",
      "0.2853016257286072\n",
      "0.2781919538974762\n",
      "0.28247588872909546\n",
      "0.2757136821746826\n",
      "0.2808836102485657\n",
      "0.28243398666381836\n",
      "0.2751452624797821\n",
      "0.27163028717041016\n",
      "0.2798360288143158\n",
      "0.2780379056930542\n",
      "0.2744850814342499\n",
      "0.27243199944496155\n",
      "0.28019437193870544\n",
      "0.27917301654815674\n",
      "0.2710762321949005\n",
      "0.2763553261756897\n",
      "0.2789969742298126\n",
      "0.26618021726608276\n",
      "0.2761209011077881\n",
      "0.26923999190330505\n",
      "0.26965540647506714\n",
      "11.249320030212402\n",
      "0.2735319137573242\n",
      "0.2720414102077484\n",
      "0.2730385959148407\n",
      "0.27098655700683594\n",
      "0.2722868025302887\n",
      "0.2707659900188446\n",
      "0.2685695290565491\n",
      "0.2643994092941284\n",
      "0.27310711145401\n",
      "0.2719419300556183\n",
      "0.2641213536262512\n",
      "0.2678014039993286\n",
      "0.266235888004303\n",
      "0.2726483643054962\n",
      "0.26460719108581543\n",
      "0.27476823329925537\n",
      "0.2711130380630493\n",
      "0.2716568410396576\n",
      "0.2663228511810303\n",
      "0.269965261220932\n",
      "0.2621198892593384\n",
      "0.267410010099411\n",
      "0.2597818374633789\n",
      "0.2654004991054535\n",
      "0.26840338110923767\n",
      "0.25853800773620605\n",
      "0.2577451169490814\n",
      "0.26392069458961487\n",
      "0.2633216083049774\n",
      "0.2596818804740906\n",
      "0.2582840025424957\n",
      "0.2641397714614868\n",
      "0.2634289562702179\n",
      "0.25609391927719116\n",
      "0.2609257400035858\n",
      "0.2638223469257355\n",
      "0.2520662248134613\n",
      "0.26152798533439636\n",
      "0.25446435809135437\n",
      "0.25706589221954346\n",
      "10.618051797151566\n",
      "0.25905412435531616\n",
      "0.2571333944797516\n",
      "0.2586584687232971\n",
      "0.256911039352417\n",
      "0.2578409016132355\n",
      "0.25691112875938416\n",
      "0.25543054938316345\n",
      "0.25097694993019104\n",
      "0.25914686918258667\n",
      "0.2581644058227539\n",
      "0.2515830993652344\n",
      "0.25466349720954895\n",
      "0.2522410750389099\n",
      "0.25480392575263977\n",
      "0.2513357102870941\n",
      "0.2560180723667145\n",
      "0.25134629011154175\n",
      "0.2521882653236389\n",
      "0.251615434885025\n",
      "0.25092995166778564\n",
      "0.24804486334323883\n",
      "0.24996282160282135\n",
      "0.24543315172195435\n",
      "0.25012892484664917\n",
      "0.25239965319633484\n",
      "0.2442893236875534\n",
      "0.2427445948123932\n",
      "0.24899572134017944\n",
      "0.2489873319864273\n",
      "0.24465055763721466\n",
      "0.2444389909505844\n",
      "0.2504245936870575\n",
      "0.24896852672100067\n",
      "0.24338027834892273\n",
      "0.24725936353206635\n",
      "0.24974793195724487\n",
      "0.23946338891983032\n",
      "0.2479056417942047\n",
      "0.2416204810142517\n",
      "0.24672745168209076\n",
      "10.03252674639225\n",
      "0.2473449409008026\n",
      "0.25089991092681885\n",
      "0.2467392235994339\n",
      "0.2475578337907791\n",
      "0.24842457473278046\n",
      "0.24796026945114136\n",
      "0.24493777751922607\n",
      "0.24213726818561554\n",
      "0.2486458569765091\n",
      "0.24766238033771515\n",
      "0.2556403875350952\n",
      "0.26336196064949036\n",
      "0.25170817971229553\n",
      "0.25089946389198303\n",
      "0.25154417753219604\n",
      "0.25796493887901306\n",
      "0.25249332189559937\n",
      "0.2502443194389343\n",
      "0.24888886511325836\n",
      "0.2484389990568161\n",
      "0.24566896259784698\n",
      "0.24759919941425323\n",
      "0.24359452724456787\n",
      "0.2471163123846054\n",
      "0.24894100427627563\n",
      "0.24142566323280334\n",
      "0.239240363240242\n",
      "0.24549107253551483\n",
      "0.24450360238552094\n",
      "0.2404658943414688\n",
      "0.23997677862644196\n",
      "0.24518315494060516\n",
      "0.24319608509540558\n",
      "0.23750625550746918\n",
      "0.24210354685783386\n",
      "0.24409523606300354\n",
      "0.23567941784858704\n",
      "0.2432238757610321\n",
      "0.23672626912593842\n",
      "0.24148251116275787\n",
      "9.856714382767677\n",
      "0.24396474659442902\n",
      "0.2407698929309845\n",
      "0.24386398494243622\n",
      "0.24327360093593597\n",
      "0.2414070963859558\n",
      "0.24214927852153778\n",
      "0.2400968074798584\n",
      "0.23566100001335144\n",
      "0.24249285459518433\n",
      "0.24151496589183807\n",
      "0.2357601523399353\n",
      "0.238412007689476\n",
      "0.23718039691448212\n",
      "0.23945611715316772\n",
      "0.2358083575963974\n",
      "0.2399595081806183\n",
      "0.2359643429517746\n",
      "0.23624677956104279\n",
      "0.23605717718601227\n",
      "0.23499439656734467\n",
      "0.23347897827625275\n",
      "0.23500783741474152\n",
      "0.2305854707956314\n",
      "0.23499682545661926\n",
      "0.23730474710464478\n",
      "0.23018072545528412\n",
      "0.228898286819458\n",
      "0.23466147482395172\n",
      "0.23535439372062683\n",
      "0.23143622279167175\n",
      "0.2307254523038864\n",
      "0.2352779656648636\n",
      "0.23370327055454254\n",
      "0.22884789109230042\n",
      "0.23297841846942902\n",
      "0.2349553108215332\n",
      "0.22732391953468323\n",
      "0.23490776121616364\n",
      "0.22886386513710022\n",
      "0.23584520816802979\n",
      "9.440367490053177\n",
      "0.23263168334960938\n",
      "0.23186443746089935\n",
      "0.2327732890844345\n",
      "0.2328127771615982\n",
      "0.23334920406341553\n",
      "0.2327149510383606\n",
      "0.2328978329896927\n",
      "0.2305089384317398\n",
      "0.23521070182323456\n",
      "0.23519004881381989\n",
      "0.23041003942489624\n",
      "0.23332816362380981\n",
      "0.23142018914222717\n",
      "0.23327253758907318\n",
      "0.2300308495759964\n",
      "0.23300260305404663\n",
      "0.2304621934890747\n",
      "0.2304021120071411\n",
      "0.23013938963413239\n",
      "0.22939492762088776\n",
      "0.2282153069972992\n",
      "0.22924253344535828\n",
      "0.22511467337608337\n",
      "0.2294131964445114\n",
      "0.2319793403148651\n",
      "0.22482077777385712\n",
      "0.22390705347061157\n",
      "0.22955207526683807\n",
      "0.22981257736682892\n",
      "0.22538095712661743\n",
      "0.22563008964061737\n",
      "0.2297888547182083\n",
      "0.22787469625473022\n",
      "0.22364109754562378\n",
      "0.22776952385902405\n",
      "0.2300548106431961\n",
      "0.22244352102279663\n",
      "0.23000578582286835\n",
      "0.22457973659038544\n",
      "0.23172567784786224\n",
      "9.192769154906273\n",
      "0.2265036702156067\n",
      "0.226455956697464\n",
      "0.22746257483959198\n",
      "0.22695329785346985\n",
      "0.22762170433998108\n",
      "0.2281675785779953\n",
      "0.22686822712421417\n",
      "0.2230532169342041\n",
      "0.22973020374774933\n",
      "0.22905468940734863\n",
      "0.2246648371219635\n",
      "0.22723659873008728\n",
      "0.22605612874031067\n",
      "0.22778117656707764\n",
      "0.2243083268404007\n",
      "0.2284056395292282\n",
      "0.22526191174983978\n",
      "0.22525529563426971\n",
      "0.22555282711982727\n",
      "0.2239661067724228\n",
      "0.22370201349258423\n",
      "0.22468072175979614\n",
      "0.22055423259735107\n",
      "0.22505882382392883\n",
      "0.22760964930057526\n",
      "0.2206028401851654\n",
      "0.21980397403240204\n",
      "0.22548018395900726\n",
      "0.2261989861726761\n",
      "0.22131510078907013\n",
      "0.22159746289253235\n",
      "0.22567687928676605\n",
      "0.2232963740825653\n",
      "0.21974876523017883\n",
      "0.223581001162529\n",
      "0.22541795670986176\n",
      "0.21881988644599915\n",
      "0.2254185676574707\n",
      "0.219956174492836\n",
      "0.22867387533187866\n",
      "8.997553437948227\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aa1529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8181\n",
      "Accuracy: 0.8218\n",
      "Accuracy: 0.8204\n",
      "Accuracy: 0.8219\n",
      "Accuracy: 0.8202\n",
      "Accuracy: 0.8244\n",
      "Accuracy: 0.8246\n",
      "Accuracy: 0.8282\n",
      "Accuracy: 0.8184\n",
      "Accuracy: 0.8197\n",
      "Accuracy: 0.8268\n",
      "Accuracy: 0.8234\n",
      "Accuracy: 0.8251\n",
      "Accuracy: 0.8218\n",
      "Accuracy: 0.8241\n",
      "Accuracy: 0.8180\n",
      "Accuracy: 0.8238\n",
      "Accuracy: 0.8215\n",
      "Accuracy: 0.8226\n",
      "Accuracy: 0.8206\n",
      "Accuracy: 0.8259\n",
      "Accuracy: 0.8245\n",
      "Accuracy: 0.8277\n",
      "Accuracy: 0.8229\n",
      "Accuracy: 0.8198\n",
      "Accuracy: 0.8278\n",
      "Accuracy: 0.8302\n",
      "Accuracy: 0.8232\n",
      "Accuracy: 0.8249\n",
      "Accuracy: 0.8251\n",
      "Accuracy: 0.8276\n",
      "Accuracy: 0.8171\n",
      "Accuracy: 0.8171\n",
      "Accuracy: 0.8261\n",
      "Accuracy: 0.8210\n",
      "Accuracy: 0.8177\n",
      "Accuracy: 0.8328\n",
      "Accuracy: 0.8209\n",
      "Accuracy: 0.8270\n",
      "Accuracy: 0.8319\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for graph_data in loader:\n",
    "        output = model(graph_data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct = pred.eq(graph_data.y).sum().item()\n",
    "        accuracy = correct / graph_data.y.size(0)\n",
    "        print('Accuracy: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2844c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
